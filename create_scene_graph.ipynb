{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "ktzdrgVrFdTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = 'data/FLUME'\n",
        "PROJECT_NAME = \"FLUME\"\n",
        "TEXT_FEATURES = [\"title\", \"description\", \"summary\"]"
      ],
      "metadata": {
        "id": "L1EPjOC2FjXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esqoqjVxFufZ",
        "outputId": "77bd3fae-7b86-4602-b9a8-d2b3660b5b08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "-tNR8YWQ6cgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_train_scene_graph(text, word2index):\n",
        "    doc = nlp(text)\n",
        "    scene_graph = []\n",
        "\n",
        "    for sentence in doc.sents:\n",
        "        subject = \"\"\n",
        "        verb = \"\"\n",
        "        object = \"\"\n",
        "        for token in sentence:\n",
        "            if token.dep_=='punct':\n",
        "              continue\n",
        "            if token not in word2index:\n",
        "              word2index[str(token)] = len(word2index)\n",
        "            if \"subj\" in token.dep_ and token.dep_!='nsubjpass':\n",
        "                subject = token.text\n",
        "            elif \"obj\" in token.dep_:\n",
        "                object = token.text\n",
        "            elif \"ROOT\" in token.dep_:\n",
        "                verb = token.text\n",
        "        if subject and verb and object:\n",
        "            scene_graph.append((subject, verb, object))\n",
        "    return scene_graph, word2index"
      ],
      "metadata": {
        "id": "pEW2J5hXGPUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "issues = pd.read_csv(\n",
        "    f\"{DATA_DIR}/train_issues.csv\")\n",
        "issues = issues.fillna(\" \")\n",
        "first_feature = TEXT_FEATURES[0]\n",
        "issues[\"text\"] = issues[first_feature]\n",
        "if len(TEXT_FEATURES)>1:\n",
        "  for feature in TEXT_FEATURES[1:]:\n",
        "    issues[\"text\"] = issues[\"text\"] + \". \" + issues[feature]"
      ],
      "metadata": {
        "id": "BBQq5JWSCgFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = issues[\"text\"].values\n",
        "keys = issues[\"Unnamed: 0\"].values"
      ],
      "metadata": {
        "id": "fKT1dFhSF2kB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scene_graphs = []\n",
        "word2index = {'<pad>': 0, '<unk>': 1, '<start>': 2, '<end>': 3}\n",
        "for text in tqdm(texts):\n",
        "  scene_graph, word2index = create_train_scene_graph(text, word2index)\n",
        "  scene_graphs.append({\"text\": text, \"rels\": scene_graph})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPWd0y56Gedc",
        "outputId": "9a295879-50e8-41a7-eba8-8a095779775f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2521/2521 [01:01<00:00, 40.91it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scene_graphs = dict(zip(keys, scene_graphs))"
      ],
      "metadata": {
        "id": "-_25Z2ZsYrlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"{DATA_DIR}/train_scene_graphs.json\", \"w\") as json_file:\n",
        "    json.dump(scene_graphs, json_file)"
      ],
      "metadata": {
        "id": "1VpQwigLIF_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"{DATA_DIR}/word2index.json\", \"w\") as json_file:\n",
        "    json.dump(word2index, json_file)"
      ],
      "metadata": {
        "id": "sQNLVbzkIiQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "4UlIq2DP6iUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_test_scene_graph(text, word2index):\n",
        "    doc = nlp(text)\n",
        "    scene_graph = []\n",
        "\n",
        "    for sentence in doc.sents:\n",
        "        subject = \"\"\n",
        "        verb = \"\"\n",
        "        object = \"\"\n",
        "        for token in sentence:\n",
        "            if token.dep_=='punct':\n",
        "              continue\n",
        "            if token not in word2index:\n",
        "              continue\n",
        "            if \"subj\" in token.dep_ and token.dep_!='nsubjpass':\n",
        "                subject = token.text\n",
        "            elif \"obj\" in token.dep_:\n",
        "\n",
        "                object = token.text\n",
        "            elif \"ROOT\" in token.dep_:\n",
        "                verb = token.text\n",
        "        if subject and verb and object:\n",
        "            scene_graph.append((subject, verb, object))\n",
        "    return scene_graph"
      ],
      "metadata": {
        "id": "oZSPGH0G6xIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "issues = pd.read_csv(\n",
        "    f\"{DATA_DIR}/test_issues.csv\")\n",
        "issues = issues.fillna(\" \")\n",
        "first_feature = TEXT_FEATURES[0]\n",
        "issues[\"text\"] = issues[first_feature]\n",
        "if len(TEXT_FEATURES)>1:\n",
        "  for feature in TEXT_FEATURES[1:]:\n",
        "    issues[\"text\"] = issues[\"text\"] + \". \" + issues[feature]"
      ],
      "metadata": {
        "id": "V74dBX_n6lc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = issues[\"text\"].values\n",
        "keys = issues[\"Unnamed: 0\"].values"
      ],
      "metadata": {
        "id": "8wYUZf5-6mrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scene_graphs = []\n",
        "with open(f\"{DATA_DIR}/word2index.json\", 'r') as file:\n",
        "    word2index = json.load(file)\n",
        "for text in tqdm(texts):\n",
        "  scene_graph = create_test_scene_graph(text, word2index)\n",
        "  scene_graphs.append({\"text\": text, \"rels\": scene_graph})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09rWpKcf6olq",
        "outputId": "d7c049c9-1f8f-4633-f6fe-9faf0e798990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 934/934 [00:32<00:00, 28.76it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scene_graphs = dict(zip(keys, scene_graphs))"
      ],
      "metadata": {
        "id": "iQR3Fth4cJO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"{DATA_DIR}/test_scene_graphs.json\", \"w\") as json_file:\n",
        "    json.dump(scene_graphs, json_file)"
      ],
      "metadata": {
        "id": "6ym-xiMw7Klp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}